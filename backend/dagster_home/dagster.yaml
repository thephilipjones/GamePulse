# Dagster instance configuration
# Configures Dagster to use PostgreSQL for run, event, and schedule storage

run_storage:
  module: dagster_postgres.run_storage
  class: PostgresRunStorage
  config:
    postgres_url:
      env: DAGSTER_POSTGRES_URL

event_log_storage:
  module: dagster_postgres.event_log
  class: PostgresEventLogStorage
  config:
    postgres_url:
      env: DAGSTER_POSTGRES_URL

schedule_storage:
  module: dagster_postgres.schedule_storage
  class: PostgresScheduleStorage
  config:
    postgres_url:
      env: DAGSTER_POSTGRES_URL

run_coordinator:
  module: dagster.core.run_coordinator
  class: QueuedRunCoordinator
  config:
    #  max_concurrent_runs: 10
    # Current: 3 concurrent runs (safe for t2.micro with async cleanup + 8 max DB connections)
    # Assets don't use "database=main" tag, so tag limit below doesn't apply
    # To enforce stricter DB concurrency: add tags={"database": "main"} to @asset decorators
    max_concurrent_runs: 3 # STRICT LIMIT for t2.micro
    tag_concurrency_limits:
      - key: "database"
        value: "main"
        limit: 1 # Only enforced if assets have tags={"database": "main"}
      # Prevent queue buildup: only 1 queued/running run per schedule
      - key: "dagster/schedule_name"
        value:
          applyLimitPerUniqueValue: true
        limit: 1

# Auto-cancel stuck runs to prevent queue buildup
run_monitoring:
  enabled: true
  start_timeout_seconds: 300    # Cancel if stuck starting for 5 minutes
  max_runtime_seconds: 600      # Cancel if running for 10 minutes
  poll_interval_seconds: 60     # Check every minute
  cancel_timeout_seconds: 120   # Allow 2 minutes for graceful cancellation

run_launcher:
  module: dagster.core.launcher
  class: DefaultRunLauncher

telemetry:
  enabled: false

retention:
  schedule:
    purge_after_days: 30
  sensor:
    purge_after_days: 30

code_servers:
  local_startup_timeout: 300 # 5 minutes for slow cold starts on t2.micro
